{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b5767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, copy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "import rexnetv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "871d0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, img_path, txt_path, transforms = None):\n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            self.img_list = [os.path.join(img_path, i.split()[0]) for i in lines]\n",
    "            self.label_list = [i.split()[1] for i in lines]\n",
    "            self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            img_path = self.img_list[index]\n",
    "            img = Image.open(img_path)\n",
    "            img = self.transforms(img)\n",
    "            label = self.label_list[index]\n",
    "        except:\n",
    "            return None\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "''' 불러온 특정 모델에 대해 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train() # 모델을 학습 상태로 지정\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        label = list(map(int, label))\n",
    "        label = torch.Tensor(label)\n",
    "        image = image.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "        label = label.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "        optimizer.zero_grad() # 기존 할당되어 있던 gradient 값 초기화\n",
    "        output = model(image) # Forward propagation\n",
    "        loss = criterion(output, label.long()) # loss 계산\n",
    "        loss.backward() # Backpropagation을 통해 계산된 gradient 값을 각 파라미터에 할당\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                  Epoch, batch_idx * len(image),\n",
    "                  len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                  loss.item()))\n",
    "\n",
    "''' 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval() # 모델을 평가 상태로 지정\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad(): # 평가하는 단계에서 gradient를 통해 파라미터 값이 업데이트되는 현상을 방지\n",
    "        for image, label in test_loader:\n",
    "            label = list(map(int, label))\n",
    "            label = torch.Tensor(label)\n",
    "            image = image.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "            label = label.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "            output = model(image) # Forward propagation\n",
    "            test_loss += criterion(output, label.long()).item() # loss 누적\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() \n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) # 평균 loss 계산\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) # 정확도 계산\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d6d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.11.0+cu113  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "''' 딥러닝 모델을 설계할 때 활요하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85259721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter 설정\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0005\n",
    "OPTIM = 'SGD' # Adam\n",
    "MODEL = 'AlexNet' # AlexNet ResNet101 ResNet50 ResNet34 ResNet18 RexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9f8a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 이미지 데이터 불러오기(Train set, Test set)'''\n",
    "# preprocessing 정의\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "}\n",
    "\n",
    "img_path = '/root/share/datasets/Bluecom'\n",
    "trainset_txt = {'fold1': './splits/Custom_Paper/fold2345_all.txt',\n",
    "                'fold2': './splits/Custom_Paper/fold1345_all.txt',\n",
    "                'fold3': './splits/Custom_Paper/fold1245_all.txt',\n",
    "                'fold4': './splits/Custom_Paper/fold1235_all.txt',\n",
    "                'fold5': './splits/Custom_Paper/fold1234_all.txt'\n",
    "            }\n",
    "validset_txt = {'fold1': './splits/Custom_Paper/fold2345_all.txt',\n",
    "            'fold2': './splits/Custom_Paper/fold1345_all.txt',\n",
    "            'fold3': './splits/Custom_Paper/fold1245_all.txt',\n",
    "            'fold4': './splits/Custom_Paper/fold1235_all.txt',\n",
    "            'fold5': './splits/Custom_Paper/fold1234_all.txt'\n",
    "        }\n",
    "testset_txt = {'fold1': './splits/Custom_Paper/fold1_all.txt',\n",
    "            'fold2': './splits/Custom_Paper/fold2_all.txt',\n",
    "            'fold3': './splits/Custom_Paper/fold3_all.txt',\n",
    "            'fold4': './splits/Custom_Paper/fold4_all.txt',\n",
    "            'fold5': './splits/Custom_Paper/fold5_all.txt',\n",
    "            }\n",
    "\n",
    "train_loader = {}\n",
    "valid_loader = {}\n",
    "test_loader = {}\n",
    "\n",
    "if MODEL == 'RexNet':\n",
    "    drop_last = True\n",
    "else:\n",
    "    drop_last = False\n",
    "\n",
    "for key in list(trainset_txt.keys()):\n",
    "    train_dataset = Data(img_path, trainset_txt[key], data_transforms['train'])\n",
    "    valid_dataset = Data(img_path, validset_txt[key], data_transforms['val'])\n",
    "    test_dataset = Data(img_path, testset_txt[key], data_transforms['val'])\n",
    "    train_loader[key] = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=drop_last)\n",
    "    valid_loader[key] = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=drop_last)\n",
    "    test_loader[key] = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "320a77b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([1, 3, 224, 224]) type: torch.FloatTensor\n",
      "y_train: 1 type: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (X_train, y_train) in train_loader['fold1']:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', len(y_train), 'type:', type(y_train))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ee0681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 5. 데이터 확인하기 (2) '''\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(BATCH_SIZE * pltsize, pltsize))\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    plt.subplot(1, BATCH_SIZE, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(X_train[i], (1, 2, 0)))\n",
    "    plt.title('Class: ' + str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922c4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "if MODEL == 'AlexNet':\n",
    "    model = models.alexnet(pretrained=False)\n",
    "    model._modules['classifier']._modules['6'] = nn.Linear(4096, 2, bias=True)\n",
    "elif MODEL == 'ResNet18':\n",
    "    model = models.resnet18(num_classes=2, pretrained=False)\n",
    "elif MODEL == 'ResNet34':\n",
    "    model = models.resnet34(num_classes=2, pretrained=False)\n",
    "elif MODEL == 'ResNet50':\n",
    "    model = models.resnet50(num_classes=2, pretrained=False)\n",
    "elif MODEL == 'ResNet101':\n",
    "    model = models.resnet101(num_classes=2, pretrained=False)\n",
    "elif MODEL == 'RexNet': # BATCH_SIZE 1은 에러남\n",
    "    model = rexnetv1.ReXNetV1(width_mult=1.0).cuda()\n",
    "    print(model(torch.randn(BATCH_SIZE, 3, 224, 224).cuda())) \n",
    "\n",
    "init_model = copy.deepcopy(model)\n",
    "print(init_model)\n",
    "\n",
    "#for children in model.classifier.children():\n",
    "#    if isinstance(children, nn.Linear):\n",
    "#        if children.out_features == 1000:\n",
    "#            print(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06b78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------    For  fold1  dataset    ------------------\n",
      "\n",
      "\n",
      "------------------    For  fold2  dataset    ------------------\n",
      "\n",
      "\n",
      "------------------    For  fold3  dataset    ------------------\n",
      "\n",
      "\n",
      "------------------    For  fold4  dataset    ------------------\n",
      "\n",
      "\n",
      "------------------    For  fold5  dataset    ------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 모델 훈련'''\n",
    "SAVE_PATH = './saved_model'\n",
    "TRAIN_LOG_PATH = './logs/train'\n",
    "TEST_LOG_PATH = './logs/test'\n",
    "if not os.path.isdir(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "if not os.path.isdir(TRAIN_LOG_PATH):\n",
    "    os.makedirs(TRAIN_LOG_PATH, exist_ok=True)\n",
    "if not os.path.isdir(TEST_LOG_PATH):\n",
    "    os.makedirs(TEST_LOG_PATH, exist_ok=True)\n",
    "\n",
    "for dataset in trainset_txt.keys():\n",
    "    print(\"\\n------------------    For \", dataset, \" dataset    ------------------\\n\")\n",
    "    tr = trainset_txt[dataset].split('splits/')[1].split('.')[0].replace('/', '-')\n",
    "    vd = validset_txt[dataset].split('splits/')[1].split('.')[0].replace('/', '-')\n",
    "\n",
    "    # Best model 경로 정의\n",
    "    if PRETRAINED:\n",
    "        MODEL_NAME = \"{}_Pretrained_BS{}_{}_LR{}_EP{}_TR-{}_VD-{}\".format(MODEL, BATCH_SIZE, OPTIM, str(LEARNING_RATE).split('.')[1], EPOCHS, tr, vd)\n",
    "    else:\n",
    "        MODEL_NAME = \"{}_BS{}_{}_LR{}_EP{}_TR-{}_VD-{}\".format(MODEL, BATCH_SIZE, OPTIM, str(LEARNING_RATE).split('.')[1], EPOCHS, tr, vd)\n",
    "    BEST_MODEL_PATH = SAVE_PATH + \"/\" + MODEL_NAME + \".pt\"\n",
    "    print(BEST_MODEL_PATH)\n",
    "\n",
    "    # 모델 초기화 \n",
    "    model = copy.deepcopy(init_model)\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Optimizer 및 loss 정의\n",
    "    if OPTIM == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    elif OPTIM == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 사전에 훈련된 모델이 존재하면 테스트만 진행하고 다음으로 넘어감 \n",
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        print(\"Trained model already exists!\\n\")\n",
    "        model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "        test_loss, test_accuracy = evaluate(model, test_loader[dataset])\n",
    "        msg = 'Test loss: {}, Test acc: {}\\n'.format(test_loss, test_accuracy)\n",
    "        print(msg)\n",
    "        continue\n",
    "\n",
    "    train_log = open(TRAIN_LOG_PATH+\"/train_log_\" + MODEL_NAME + \".txt\", 'w')\n",
    "\n",
    "    best_acc = 0\n",
    "    best_ep = 0\n",
    "\n",
    "    for Epoch in range(1, EPOCHS + 1):\n",
    "        train(model, train_loader[dataset], optimizer, log_interval = 200)\n",
    "        valid_loss, valid_accuracy = evaluate(model, valid_loader[dataset])\n",
    "        if valid_accuracy > best_acc:\n",
    "            best_acc = valid_accuracy\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_ep = Epoch\n",
    "            msg = \"Best Model!\\n\"\n",
    "            print(msg)\n",
    "            train_log.writelines(msg)\n",
    "        msg = \"\\nEPOCH: {}], \\tValidation Loss: {:.4f}, \\tValidation Accuracy: {:.2f} %\\n\".format(Epoch, valid_loss, valid_accuracy)\n",
    "        print(msg)\n",
    "        train_log.writelines(msg)\n",
    "\n",
    "    torch.save(best_model.state_dict(), BEST_MODEL_PATH)\n",
    "    msg = \"\\n------------------    Best Model at Epoch {} Saved    ------------------\\n\".format(best_ep)\n",
    "    print(msg)\n",
    "    train_log.writelines(msg)\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader[dataset])\n",
    "    msg = '\\nTest loss: {}, Test acc: {}\\n'.format(test_loss, test_accuracy)\n",
    "    print(msg)\n",
    "    train_log.writelines(msg)\n",
    "\n",
    "    train_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9d8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*--------------------- Test Result ---------------------*\n",
      "\n",
      "fold1 - loss: 0.1524992298941298, acc: 96.78111587982832\n",
      "\n",
      "fold2 - loss: 0.11382482364478698, acc: 97.19222462203024\n",
      "\n",
      "fold3 - loss: 0.08616235242097432, acc: 98.06451612903226\n",
      "\n",
      "fold4 - loss: 0.06367176929724273, acc: 98.30866807610994\n",
      "\n",
      "fold5 - loss: 0.06943563478255071, acc: 97.47899159663865\n",
      "\n",
      "average accuracy: 97.56510326072788\n"
     ]
    }
   ],
   "source": [
    "''' 훈련된 모델 확인하기 '''\n",
    "if PRETRAINED:\n",
    "    TEST_NAME = \"{}_Pretrained_BS{}_{}_LR{}_EP{}_{}\".format(MODEL, BATCH_SIZE, OPTIM, str(LEARNING_RATE).split('.')[1], EPOCHS, datetime.now())\n",
    "else:\n",
    "    TEST_NAME = \"{}_BS{}_{}_LR{}_EP{}_{}\".format(MODEL, BATCH_SIZE, OPTIM, str(LEARNING_RATE).split('.')[1], EPOCHS, datetime.now())\n",
    "test_log = open(TEST_LOG_PATH+\"/test_log_\" + TEST_NAME + \".txt\", 'w')\n",
    "test_log.writelines(\"Dataset List\\n\")\n",
    "for dataset in trainset_txt.keys():\n",
    "    test_log.writelines(\"- {} train: {}, test: {} \\n\".format(dataset, trainset_txt[dataset], testset_txt[dataset]))\n",
    "\n",
    "msg = \"\\n*--------------------- Test Result ---------------------*\\n\"\n",
    "print(msg)\n",
    "test_log.writelines(msg)\n",
    "\n",
    "total_loss = 0\n",
    "for dataset in trainset_txt.keys():\n",
    "    tr = trainset_txt[dataset].split('splits/')[1].split('.')[0].replace('/', '-')\n",
    "    vd = validset_txt[dataset].split('splits/')[1].split('.')[0].replace('/', '-')\n",
    "    if PRETRAINED:\n",
    "        MODEL_NAME = \"{}_Pretrained_BS{}_{}_LR{}_EP{}_TR-{}_VD-{}\".format(MODEL, BATCH_SIZE, OPTIM, str(LEARNING_RATE).split('.')[1], EPOCHS, tr, vd)\n",
    "    else:\n",
    "        MODEL_NAME = \"{}_BS{}_{}_LR{}_EP{}_TR-{}_VD-{}\".format(MODEL, BATCH_SIZE, OPTIM, str(LEARNING_RATE).split('.')[1], EPOCHS, tr, vd)\n",
    "    BEST_MODEL_PATH = SAVE_PATH + \"/\" + MODEL_NAME + \".pt\"\n",
    "\n",
    "    if MODEL == 'AlexNet':\n",
    "        trained_model = models.alexnet(pretrained=False)\n",
    "        trained_model._modules['classifier']._modules['6'] = nn.Linear(4096, 2, bias=True)\n",
    "    elif MODEL == 'ResNet18':\n",
    "        trained_model = models.resnet18(num_classes=2, pretrained=False)\n",
    "    elif MODEL == 'ResNet34':\n",
    "        trained_model = models.resnet34(num_classes=2, pretrained=False)\n",
    "    elif MODEL == 'ResNet50':\n",
    "        trained_model = models.resnet50(num_classes=2, pretrained=False)\n",
    "    elif MODEL == 'ResNet101':\n",
    "        trained_model = models.resnet101(num_classes=2, pretrained=False)\n",
    "    elif MODEL == 'RexNet': # BATCH_SIZE 1은 에러남\n",
    "        trained_model = rexnetv1.ReXNetV1(width_mult=1.0).cuda()\n",
    "\n",
    "    trained_model = trained_model.cuda()\n",
    "    trained_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(trained_model, test_loader[dataset])\n",
    "    total_loss += test_accuracy\n",
    "    msg = '{} - loss: {}, acc: {}\\n'.format(dataset, test_loss, test_accuracy)\n",
    "    print(msg)\n",
    "    test_log.writelines(msg)\n",
    "\n",
    "total_loss /= len(trainset_txt)\n",
    "msg = \"average accuracy: {}\".format(total_loss)\n",
    "print(msg)\n",
    "test_log.writelines(msg)\n",
    "\n",
    "test_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e8ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
