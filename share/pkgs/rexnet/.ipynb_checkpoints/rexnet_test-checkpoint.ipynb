{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac20ded1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReXNetV1(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU()\n",
       "    (3): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6()\n",
       "        (6): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(27, 162, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(162, 162, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=162, bias=False)\n",
       "        (4): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6()\n",
       "        (6): Conv2d(162, 38, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(38, 228, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(228, 228, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=228, bias=False)\n",
       "        (4): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(228, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(19, 228, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(228, 50, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(50, 300, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(300, 300, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=300, bias=False)\n",
       "        (4): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(300, 25, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(25, 300, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(300, 61, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(61, 366, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(366, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(366, 366, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=366, bias=False)\n",
       "        (4): BatchNorm2d(366, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(366, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(30, 366, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(366, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(72, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(432, 432, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=432, bias=False)\n",
       "        (4): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(432, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(36, 432, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(432, 84, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(84, 504, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(504, 504, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=504, bias=False)\n",
       "        (4): BatchNorm2d(504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(504, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(42, 504, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(504, 95, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(95, 570, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(570, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(570, 570, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=570, bias=False)\n",
       "        (4): BatchNorm2d(570, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(570, 47, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(47, 570, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(570, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(106, 636, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(636, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(636, 636, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=636, bias=False)\n",
       "        (4): BatchNorm2d(636, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(636, 53, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(53, 636, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(636, 117, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(117, 702, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(702, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(702, 702, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=702, bias=False)\n",
       "        (4): BatchNorm2d(702, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(702, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(58, 702, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(702, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=768, bias=False)\n",
       "        (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(64, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(768, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(140, 840, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(840, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(840, 840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=840, bias=False)\n",
       "        (4): BatchNorm2d(840, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(840, 70, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(70, 840, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(840, 151, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(151, 906, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(906, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(906, 906, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=906, bias=False)\n",
       "        (4): BatchNorm2d(906, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(906, 75, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(75, 906, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(906, 162, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(162, 972, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(972, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(972, 972, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=972, bias=False)\n",
       "        (4): BatchNorm2d(972, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(972, 81, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(81, 972, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(972, 174, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): LinearBottleneck(\n",
       "      (out): Sequential(\n",
       "        (0): Conv2d(174, 1044, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1044, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(1044, 1044, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1044, bias=False)\n",
       "        (4): BatchNorm2d(1044, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SE(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Conv2d(1044, 87, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(87, 1044, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (6): ReLU6()\n",
       "        (7): Conv2d(1044, 185, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(185, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (19): Conv2d(185, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (20): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): SiLU()\n",
       "    (22): AdaptiveAvgPool2d(output_size=1)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Conv2d(1280, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import rexnetv1\n",
    "\n",
    "model = rexnetv1.ReXNetV1(width_mult=1.0).cuda()\n",
    "model.load_state_dict(torch.load('./rexnetv1_1.0.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4243d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!echo $TORCH_CUDA_ARCH_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4250c2f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/root/share/pkgs/rexnet/rexnetv1.py\", line 18, in silu_fwd\n    @torch.jit.script\n    def silu_fwd(x):\n        return x.mul(torch.sigmoid(x))\n                     ~~~~~~~~~~~~~ <--- HERE\nRuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/share/pkgs/rexnet/rexnetv1.py:184\u001b[0m, in \u001b[0;36mReXNetV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 184\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/share/pkgs/rexnet/rexnetv1.py:53\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/share/pkgs/rexnet/rexnetv1.py:40\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(x, inplace)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msilu\u001b[39m(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSiLUJitImplementation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/share/pkgs/rexnet/rexnetv1.py:31\u001b[0m, in \u001b[0;36mSiLUJitImplementation.forward\u001b[0;34m(ctx, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, x):\n\u001b[1;32m     30\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(x)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msilu_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/root/share/pkgs/rexnet/rexnetv1.py\", line 18, in silu_fwd\n    @torch.jit.script\n    def silu_fwd(x):\n        return x.mul(torch.sigmoid(x))\n                     ~~~~~~~~~~~~~ <--- HERE\nRuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.randn(1, 3, 224, 224).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8120e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
